---
title: "How do we know how smart AI systems are?"
layout: "Article"
tags: 
  -Definition
  -IT
  -News
  -Tech
  -Technologies
  -Africa
  -Funding
excerpt:
publishedAt:
---
<br/>
In a recent article published in Science, Melanie Mitchell discusses the current state of artificial intelligence (AI) and the challenges in assessing the true intelligence of AI systems. The article starts by mentioning the optimistic prediction made by Marvin Minsky in 1967 about solving the problem of creating human-level AI within a generation. While some leading AI researchers today believe we are getting close to that goal, the author emphasizes the need for extraordinary evidence to support such claims.
<br/>
The article highlights the issue of anthropomorphism, wherein humans tend to project intelligence onto AI systems that display linguistic competence, even though their understanding may be limited. The use of standardized tests to assess AI intelligence is also questioned due to potential data contamination and the lack of robustness in AI systems' responses.
<br/>
The author calls for a more rigorous approach to evaluating AI capabilities, akin to the methods used to assess human intelligence in children. This involves testing AI systems on multiple variations of tasks and underlying concepts to truly understand their comprehension.
<br/>
The article concludes that AI systems, especially large language models like GPT-4, are becoming increasingly influential in society. To ensure accurate assessments of their intelligence and limitations, more transparency in their training methods and collaboration between AI researchers and cognitive scientists is necessary.
<br/>
While AI has made significant advancements, accurately determining its true intelligence requires more robust evaluation methods and a deeper understanding of AI systems' capabilities.
<br/>
<Button
  address="https://www.science.org/doi/10.1126/science.adj5957?utm_source=pocket_saves"
  text="Source"
/>
<br/>
<br/>
<br/>
<br/>
# Others Keys Reads
<br/>
### ***GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE***
<br/>
<Image src="/OpenAI-logo-symmetry.jpeg" alt="" width={600} height={500} />
<br />
The text discusses OpenAI's GPT-4 model architecture and the challenges they faced while scaling the model. The main goal of GPT-4 was to scale 100 times from GPT-3 while still ensuring a manageable cost. Dense transformer models like GPT-3 were found to be unsuitable for further scaling due to the increasing training cost. However, the cost turned out to be less relevant because major companies like Google, Meta, and OpenAI/Microsoft are willing to spend vast amounts of money on training supercomputers to develop massive models with tangible value.
<br/>
The primary challenge lies in inference rather than training. Inference costs far exceed training costs. OpenAI's innovation targets focused on decoupling training compute from inference compute. They achieved this by using sparse model architecture, where not every parameter is activated during inference. This allowed OpenAI to offer GPT-4, with a model larger than 1 trillion parameters, at a low price of $0.06 per 1,000 tokens, achieving human reading speed.
<br/>
While the model architecture and training infrastructure are impressive, the real struggle lies in scaling out the models to users and agents due to high inference costs. Large language models require massive memory bandwidth, and even the latest Nvidia H100 GPU servers struggle to meet the required throughput. OpenAI managed to overcome this challenge by employing sparse architecture, making GPT-4 efficient and cost-effective.
<br/>
The text also invites readers to meet up during ICML in Hawaii and offers a 20% discount on a group subscription.
<br/>
<Button
  address="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure"
  text="Source"
/>