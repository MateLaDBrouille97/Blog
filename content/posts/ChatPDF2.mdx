---
title: "Chat with your PDF"
layout: "Article"
tags:
  -AI 
  -IT
  -News
  -Chat PDF
 
excerpt:
publishedAt:
---
<br/>
Langchain is a Python library that offers a range of tools for natural language processing (N.L.P.) tasks. It enables text splitting, embedding generation, and integration with powerful N.L.P. models like OpenAI's GPT-3.5. F.A.I.S.S., on the other hand, is a library designed for efficient similarity search and clustering of dense vectors.
<br/>
In our chat feature, we'll utilize Langchain to split the text from the PDF into smaller portions, convert them into embeddings using OpenAIEmbeddings, and establish a knowledge base using F.A.I.S.S. This knowledge base enables us to efficiently search for similar information when a user poses a question. For question-answering, we'll utilize OpenAI's GPT-3.5 model through Langchain's L.L.M.S. (Large Language Model as a Service) functionality.
<br/>
# Dependencies
<br/>
Let's install the necessary dependencies. We'll be using the following libraries:
<br/>
<br/>
dotenv: For loading the OpenAi Api Key
<br/>
PyPDF2: For reading PDF files
<br/>
streamlit: For building the user interface
<br/>
langchain: For text splitting, embeddings, F.A.I.S.S. integration, and question-answering
<br/>
openai: For accessing the OpenAI GPT-3.5 model
<br/>
<br/>

```python
pip install python-dotenv PyPDF2 streamlit langchain openai

```
<br/>
<br/>
# Implementing the Chat Functionality
<br/>

```python
from dotenv import load_dotenv
import os
from PyPDF2 import PdfReader
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
Load environment variables
load_dotenv()

```
<br/>
<br/>
First, we'll create a function called "process_text" to break down the text into smaller parts using Langchain's CharacterTextSplitter. These parts will be transformed into embeddings using OpenAIEmbeddings. After that, we'll build a knowledge base using F.A.I.S.S. and finally return it.
<br/>

```python
def process_text(text):
    # Split the text into chunks using Langchain's CharacterTextSplitter
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    
    # Convert the chunks of text into embeddings to form a knowledge base
    embeddings = OpenAIEmbeddings()
    knowledgeBase = FAISS.from_texts(chunks, embeddings)
    
    return knowledgeBase
```
<br/>
<br/>
Now, we'll create the main function called "main" where we'll develop the user interface using Streamlit. To begin, we'll set the title of the app. Then, we'll include a file uploader allowing users to upload PDF documents. If a PDF is uploaded, we'll use PyPDF2's PdfReader to read its content and save it in the "text" variable. Next, we'll process the text to generate a knowledge base using the earlier defined "process_text" function. If the user asks a question, we'll perform a similarity search on the knowledge base to retrieve relevant documents based on the user's query.
<br/>

```python
def main():
    st.title("Chat with your PDF ðŸ’¬")
    
    pdf = st.file_uploader('Upload your PDF Document', type='pdf')
    
    if pdf is not None:
        pdf_reader = PdfReader(pdf)
        # Text variable will store the pdf text
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        
        # Create the knowledge base object
        knowledgeBase = process_text(text)
        
        query = st.text_input('Ask a question to the PDF')
        cancel_button = st.button('Cancel')
        
        if cancel_button:
            st.stop()
        
        if query:
            docs = knowledgeBase.similarity_search(query)
```
<br/>
<br/>
Moving forward, we begin by setting up the L.L.M.S. (Large Language Model as a Service) utilizing OpenAI's GPT-3.5 model. Additionally, we load the question-answering chain by employing Langchain's load_qa_chain function, specifying the L.L.M.S. instance and the chain type as 'stuff.' To manage costs, we create a context manager with get_openai_callback. Lastly, we present the user's response in the Streamlit app.
<br/>

```python
 llm = OpenAI()
            chain = load_qa_chain(llm, chain_type='stuff')
            
            with get_openai_callback() as cost:
                response = chain.run(input_documents=docs, question=query)
                print(cost)
                
            st.write(response)
            
            
if __name__ == "__main__":
    main()
```
<br/>
<br/>
<br/>
<Image src="/1_hOs-4a7ZCYuS9AOBneEODw.jpg" alt="" width={600} height={500} />
<br/>
In summary, we've explored the implementation of a chat feature that allows users to query PDF documents using Langchain, F.A.I.S.S., and the OpenAI API. By utilizing techniques like text splitting, embeddings, and question-answering, we can create an interactive chat interface for extracting information from PDFs. This approach can be tailored and enhanced to meet specific needs, serving as a valuable tool for retrieving information and extracting knowledge from PDF documents.
<br/>
<br/>
<br/>
<br/>
<br/>
