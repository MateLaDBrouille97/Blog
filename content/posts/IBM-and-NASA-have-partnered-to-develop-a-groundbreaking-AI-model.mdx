---
title: "IBM and NASA have partnered to develop a groundbreaking AI model known as the GPT for Earth Sciences"
layout: "Article"
tags: 
  -Definition
  -IT
  -News
  -Tech
  -AI
  -Groundbreaking AI 
  -Model
  -Earth Sciences
  
excerpt:
publishedAt:
---
<br/>
IBM and NASA have collaboratively created the "GPT of Earth Sciences," a groundbreaking AI model designed for analyzing Earth science data. Utilizing IBM's foundation model technology, which incorporates a vast dataset of text and code, the model will process and analyze large volumes of Earth science data, including satellite imagery, climate, and meteorological data. This partnership combines NASA's expertise in space research with IBM's cutting-edge AI technology, allowing researchers to analyze complex data in real-time and uncover hidden patterns and trends related to the planet's climate, weather, and natural resources.
<br/>
The potential applications of the GPT of Earth Sciences are diverse. Environmental scientists can leverage the technology to forecast the impacts of climate change, improve disaster response capabilities, and develop long-term land management and conservation plans. Additionally, the AI model can aid in the early detection and monitoring of ecological issues like wildfires, deforestation, and coral reef degradation. Policymakers and government agencies can make informed decisions on resource allocation, environmental regulations, and disaster planning by utilizing this advanced AI technology.
<br/>
Beyond its scientific applications, the model holds promise for creating interactive maps showcasing Earth's changing climate over time and generating fresh educational materials and public applications. This partnership between IBM and NASA marks a significant advancement in the development of AI for Earth, offering potential solutions to some of the planet's most pressing environmental challenges across various industries.
<br/>
<Button
  address="https://industrywired.com/gpt-for-earth-sciences-by-ibm-and-nasa/"
  text="Source"
/>
<br/>
<br/>
<br/>
<br/>
# Others Keys Reads
<br/>
# ***AI search of Neanderthal proteins resurrects ‘extinct’ antibiotics***
<br/>
<Image src="/d41586-023-02403-0_25858106.jpeg" alt="" width={600} height={500} />
<br />
Bioengineers have utilized artificial intelligence (AI) to resurrect ancient molecules, particularly peptides, from extinct species like Neanderthals and Denisovans. These peptides possess antimicrobial properties and could serve as a resource for developing new antibiotics to combat drug-resistant bacteria. With antibiotic development slowing down and antibiotic-resistant bacteria on the rise, finding new treatments is crucial.
<br/>
The researchers trained an AI algorithm to recognize specific sites on human proteins where peptides are known to be cut. By applying this algorithm to protein sequences of modern humans, Neanderthals, and Denisovans, they identified potential peptides with antimicrobial properties. Testing dozens of these peptides, the team found six potent candidates, four from modern humans, one from Neanderthals, and one from Denisovans. Although all six peptides inhibited the growth of Acinetobacter baumannii, a bacterium causing hospital-borne infections, they did not completely eliminate the bacteria.
<br/>
Despite the peptides' limitations, the researchers believe that tweaking them and improving the AI algorithm could lead to more effective drug candidates. However, some experts remain cautious, stating that the approach needs to achieve better success rates before it significantly impacts drug discovery. Nevertheless, the study represents a new avenue for antibiotic development and has garnered interest from researchers in the field.
<br/>
<Button
  address="https://www.nature.com/articles/d41586-023-02403-0"
  text="Source"
/>
<br/>
<br/>
<br/>
# ***Trustworthy AI for safe medicines***
<br/>
The authors support the idea that grading requirements for pharmaceutical AI should align with the risk level of the application. They agree with Hines et al.'s focus on AI applications' impact on the risk-benefit ratio of medicines but emphasize the importance of categorization based on use context across the entire enterprise. High-risk applications should adhere to strict quality management standardized by industry regulators, while low-risk applications in research and discovery, not affecting patient rights, only need to follow good machine learning practices.
<br/>
The regulatory framework for pharmaceutical AI should harmonize with existing pharmaceutical regulations rather than impose uniform regulations across different fields. This approach ensures that AI regulations cater to the specific needs of the pharmaceutical industry and avoids potential conflicts and uncertainty.
<br/>
Regarding safe and trusted use of AI, the authors propose focusing on the outcomes of validation measures throughout the drug development lifecycle. They argue against the need for 'white box' access to algorithms and datasets, as this may inhibit innovation. Instead, they advocate for a 'black box' assessment combined with methodological transparency and outcome validation to ensure medicine safety without compromising innovation. Transparency tools like datasheets and summaries can provide insights into datasets without requiring full access to algorithms and data.
<br/>
The authors suggest that a quality-management approach should be adopted to facilitate risk-based innovation and maximize appropriate use of data wherever it resides. They encourage the European Medicines Agency (EMA) to engage actively with industry and representative bodies to establish a harmonized, pro-innovation regulatory framework for pharmaceutical AI, taking advantage of decentralized data networks and AI-based technologies for pharmacovigilance.
<br/>
<Button
  address="https://www.nature.com/articles/s41573-023-00769-4"
  text="Source"
/>