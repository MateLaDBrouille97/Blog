---
title: "All languages are NOT created (tokenized) equal"
layout: "Article"
tags: 
  -Definition
  -IT
  -News
  -Tech
  -Technologies
  -Languages
  -tokens

excerpt:
publishedAt:
---
<br/>
To understand the variations in tokenization, Yennie JUN conducted an analysis on a parallel dataset consisting of short messages translated into 52 different languages, utilizing the vast Amazon MASSIVE dataset. The findings were intriguing. It was discovered that the number of tokens required for a given text can differ greatly depending on the language involved. For instance, English texts typically have smaller median token lengths, while languages like Burmese demand a significantly larger number of tokens to represent the same information.
<br/>
To provide a clearer picture of these disparities, Yennie JUN presents a chart showcasing the median token lengths for a subset of languages. This visual representation effectively highlights the differences in tokenization requirements among various languages.
<br/>
The implications of these disparities are profound. Firstly, they impose limitations on the amount of information that can be included in a given prompt. Secondly, they result in increased costs and longer processing times, making NLP applications less efficient and accessible for languages requiring more tokens. This is particularly significant in countries where English is not the dominant language, emphasizing the urgency of addressing these inequalities.
<br/>
Furthermore, the article draws attention to the digital divide in NLP research. Despite the dominance of non-English languages on the internet, a majority of studies focus predominantly on English. This imbalance not only perpetuates disparities but also affects the performance of multilingual models on low-resource languages. Consequently, the article stresses the need to prioritize the development of low-resource languages, promoting equitable representation and performance in AI-driven technologies.
<br/>
To illustrate the historical context of language disparities, Yennie JUN draws parallels with past instances such as telegraphy systems. Similarly, modern NLP models, largely trained on English, face challenges in processing non-Latin script languages. By highlighting these parallels, the article encourages a deeper understanding of the issue and the urgency to rectify it.
<br/>
Addressing language disparities in NLP is of utmost importance to foster inclusivity and accessibility in AI technologies. By investing in further research, collaboration, and the development of low-resource languages, we can create a more diverse and equitable linguistic landscape in AI applications. JUN  concludes by providing a compelling visualization of different languages and their corresponding token lengths, emphasizing the complexities involved in rendering multiple language scripts and fonts.
<br/>
<Button
  address="https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized"
  text="Source"
/>